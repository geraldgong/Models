{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T18:16:18.121882Z",
     "start_time": "2021-01-27T18:16:16.103769Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T19:11:24.530761Z",
     "start_time": "2021-01-27T19:11:24.397292Z"
    }
   },
   "outputs": [],
   "source": [
    "# Sentence based data instead of word based\n",
    "data = open('../data/sonnets.txt').read()\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "# Fit on texts first then convert text to digital\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.index_word) + 1\n",
    "\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(input_squences,\n",
    "                                 maxlen=max_sequence_len,\n",
    "                                 padding='pre'\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T15:39:07.158389Z",
     "start_time": "2021-01-27T15:39:07.154839Z"
    }
   },
   "source": [
    "# Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T19:11:27.272555Z",
     "start_time": "2021-01-27T19:11:27.229626Z"
    }
   },
   "outputs": [],
   "source": [
    "predictors, label = padded_sequences[:, :-1], padded_sequences[:, -1]\n",
    "label = tf.keras.utils.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T19:11:30.400518Z",
     "start_time": "2021-01-27T19:11:29.896067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-27T19:11:53.534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "484/484 [==============================] - 20s 41ms/step - loss: 6.5385 - accuracy: 0.0207\n",
      "Epoch 2/10\n",
      "484/484 [==============================] - 20s 41ms/step - loss: 6.4325 - accuracy: 0.0241\n",
      "Epoch 3/10\n",
      "484/484 [==============================] - 20s 41ms/step - loss: 6.3156 - accuracy: 0.0282\n",
      "Epoch 4/10\n",
      "484/484 [==============================] - 58s 119ms/step - loss: 6.2119 - accuracy: 0.0303\n",
      "Epoch 5/10\n",
      "484/484 [==============================] - 24s 49ms/step - loss: 6.1259 - accuracy: 0.0384\n",
      "Epoch 6/10\n",
      "484/484 [==============================] - 22s 46ms/step - loss: 6.0422 - accuracy: 0.0395\n",
      "Epoch 7/10\n",
      "484/484 [==============================] - 29s 60ms/step - loss: 5.9665 - accuracy: 0.0409\n",
      "Epoch 8/10\n",
      "484/484 [==============================] - 25s 52ms/step - loss: 5.8712 - accuracy: 0.0460\n",
      "Epoch 9/10\n",
      "423/484 [=========================>....] - ETA: 2s - loss: 5.7624 - accuracy: 0.0540"
     ]
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-27T19:11:59.113Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-27T19:12:01.013Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 10\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-env-models",
   "language": "python",
   "name": "conda-env-models"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
